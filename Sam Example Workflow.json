{"id":"00000000-0000-0000-0000-000000000000","revision":0,"last_node_id":30,"last_link_id":26,"nodes":[{"id":1,"type":"CheckpointLoaderSimple","pos":[100,130],"size":[315,98],"flags":{},"order":0,"mode":0,"inputs":[],"outputs":[{"localized_name":"MODEL","name":"MODEL","type":"MODEL","links":[25,26]},{"localized_name":"CLIP","name":"CLIP","type":"CLIP","links":[1]},{"localized_name":"VAE","name":"VAE","type":"VAE","links":[10,12,17]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"CheckpointLoaderSimple"},"widgets_values":["epiCRealismXL-LastFAME"]},{"id":2,"type":"CLIPSetLastLayer","pos":[515,130],"size":[315,58],"flags":{},"order":18,"mode":0,"inputs":[{"localized_name":"clip","name":"clip","type":"CLIP","link":1}],"outputs":[{"localized_name":"CLIP","name":"CLIP","type":"CLIP","links":[2,3]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"CLIPSetLastLayer"},"widgets_values":[-1]},{"id":3,"type":"CLIPTextEncode","pos":[930,130],"size":[400,200],"flags":{},"order":19,"mode":0,"inputs":[{"localized_name":"clip","name":"clip","type":"CLIP","link":2}],"outputs":[{"localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","links":[5,19]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"CLIPTextEncode"},"widgets_values":["analog film photo. hip-hop theme, urban, close-up shot, dynamic portrait of a breathtaking Basotho model with a small smile wearing an oversized baggy and loose tracksuit in a cluttered warehouse, (puffy eyes:1.2), detailed face, stunning beauty, detailed skin, skin texture, skin pores, (freckles:0.9), bling bling, drip drip. Hard light, hard shadows, faded film, desaturated, 35mm photo, grainy, vignette, vintage, Kodachrome, Lomography, stained, highly detailed, found footage"]},{"id":4,"type":"CLIPTextEncode","pos":[930,460],"size":[400,200],"flags":{},"order":20,"mode":0,"inputs":[{"localized_name":"clip","name":"clip","type":"CLIP","link":3}],"outputs":[{"localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","links":[6,20]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"CLIPTextEncode"},"widgets_values":["ugly, child, kid, face paint, tattoo, dirty, messy, nude, watermark, monochrome"]},{"id":6,"type":"EmptyLatentImage","pos":[100,358],"size":[315,106],"flags":{},"order":1,"mode":0,"inputs":[],"outputs":[{"localized_name":"LATENT","name":"LATENT","type":"LATENT","links":[7]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"EmptyLatentImage"},"widgets_values":[896,1152,1]},{"id":7,"type":"VAEDecode","pos":[3945.199951171875,130],"size":[210,46],"flags":{},"order":27,"mode":0,"inputs":[{"localized_name":"samples","name":"samples","type":"LATENT","link":22},{"localized_name":"vae","name":"vae","type":"VAE","link":10}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[8]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"VAEDecode"},"widgets_values":[]},{"id":8,"type":"SaveImage","pos":[4255.2001953125,130],"size":[315,58],"flags":{},"order":28,"mode":0,"inputs":[{"localized_name":"images","name":"images","type":"IMAGE","link":8}],"outputs":[],"properties":{"cnr_id":"comfy-core","ver":"0.3.27"},"widgets_values":["ComfyUI"]},{"id":9,"type":"VAEDecodeTiled","pos":[1845,130],"size":[315,150],"flags":{},"order":22,"mode":0,"inputs":[{"localized_name":"samples","name":"samples","type":"LATENT","link":11},{"localized_name":"vae","name":"vae","type":"VAE","link":12}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[13]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"VAEDecodeTiled"},"widgets_values":[512,64,64,8]},{"id":11,"type":"ImageUpscaleWithModel","pos":[2260,130],"size":[340.20001220703125,46],"flags":{},"order":23,"mode":0,"inputs":[{"localized_name":"upscale_model","name":"upscale_model","type":"UPSCALE_MODEL","link":14},{"localized_name":"image","name":"image","type":"IMAGE","link":13}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[15]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"ImageUpscaleWithModel"},"widgets_values":[]},{"id":13,"type":"VAEEncodeTiled","pos":[3115.199951171875,130],"size":[315,150],"flags":{},"order":25,"mode":0,"inputs":[{"localized_name":"pixels","name":"pixels","type":"IMAGE","link":16},{"localized_name":"vae","name":"vae","type":"VAE","link":17}],"outputs":[{"localized_name":"LATENT","name":"LATENT","type":"LATENT","links":[21]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"VAEEncodeTiled"},"widgets_values":[512,64,64,8]},{"id":14,"type":"KSampler","pos":[3530.199951171875,130],"size":[315,262],"flags":{},"order":26,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":26},{"localized_name":"positive","name":"positive","type":"CONDITIONING","link":19},{"localized_name":"negative","name":"negative","type":"CONDITIONING","link":20},{"localized_name":"latent_image","name":"latent_image","type":"LATENT","link":21}],"outputs":[{"localized_name":"LATENT","name":"LATENT","type":"LATENT","links":[22]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"KSampler"},"widgets_values":[0,"randomize",15,3,"euler","normal",0.3]},{"id":16,"type":"MarkdownNote","pos":[-257.9193115234375,358.8603515625],"size":[330.09490966796875,107.0767822265625],"flags":{},"order":2,"mode":0,"inputs":[],"outputs":[],"title":"Empty Latent Image","properties":{},"widgets_values":["## This loads in an empty image in latent format instead of normal pixels to speed things up and more. Standard for all generation nowadays."],"color":"#432","bgcolor":"#653"},{"id":17,"type":"MarkdownNote","pos":[-290.0983581542969,528.6943969726562],"size":[374.7880859375,157.1331329345703],"flags":{},"order":3,"mode":0,"inputs":[],"outputs":[],"title":"Load Upscale Model","properties":{},"widgets_values":["## This loads in an upscaler model which generally refines the pixels in an image to give it more detail and increases resolution as well by doing so. Think of this as a baby checkpoint that is trained on tiny parts of images instead of entire images. We can mess around with this a bit to achieve desired results"],"color":"#432","bgcolor":"#653"},{"id":18,"type":"MarkdownNote","pos":[452.4041442871094,-180.43711853027344],"size":[458.2153015136719,269.1639709472656],"flags":{},"order":4,"mode":0,"inputs":[],"outputs":[],"title":"CLIP Set Last Layer","properties":{},"widgets_values":["## The CLIP translation has layers. The deeper it goes the more related words it will find. For example, a 3 layer description of \"hand\" could be:\n                   __ Hand __\n                  /          \\\n            _thumb_            _fingers_\n           /       \\          /         \\\n      nail     knuckles      nails       knuckles\n    /      \\      /     \\    /    \\     /       \\         \n## You get the gist. Removing layers will allow the algorithm to be more creative but could lead to more hallucinations and too many layers could lead to overspecification and crappy images"],"color":"#432","bgcolor":"#653"},{"id":20,"type":"MarkdownNote","pos":[928.101806640625,703.2955932617188],"size":[411.5718994140625,142.56045532226562],"flags":{},"order":5,"mode":0,"inputs":[],"outputs":[],"title":"CLIP Text Encode (Negative)","properties":{},"widgets_values":["## The bottom text encode is the negative prompting (what we DON'T want to see). Depending on the model, negative prompting can actually make things worse. Try to keep things short and sweet. For example, if you don't want to see a naked girl just write nude and leave it at that"],"color":"#432","bgcolor":"#653"},{"id":19,"type":"MarkdownNote","pos":[970.8448486328125,-30.86403465270996],"size":[296.7240295410156,110.05634307861328],"flags":{},"order":6,"mode":0,"inputs":[],"outputs":[],"title":"CLIP Text Encode (Positive)","properties":{},"widgets_values":["## The top text encode is the positive prompting (what we want to see)"],"color":"#432","bgcolor":"#653"},{"id":15,"type":"MarkdownNote","pos":[-458.7405090332031,-120.25039672851562],"size":[535.0875854492188,397.88031005859375],"flags":{},"order":7,"mode":0,"inputs":[],"outputs":[],"title":"Load Checkpoint","properties":{},"widgets_values":["# This loads any kind of checkpoint. This is what we talk about when we say the \"model\" we are working with. Examples are Flux or epiCrealism etc. Some have built in VAE/CLIPs:\n## - VAE is used to translate latent pixels (randomized crappy pixels that hold less data and make generation faster) into real pixels that computers use. Think of this as compressing the image, except instead of just making the file size smaller it also compresses the information the individual pixels hold. Generally we don't want to mess with this too much as the built in VAEs are made for that model and won't misinterpret latent pixels\n## - CLIPs are the translator which takes our human words and turns them into vector format for the machine learning algorithm to understand (it's actually super interesting how this is done, too bad Professor Wood was garbage at teaching linear). Sometimes we can mess around with this, some CLIPs can give more emphasis to certain words etc"],"color":"#432","bgcolor":"#653"},{"id":5,"type":"KSampler","pos":[1430,130],"size":[315,262],"flags":{},"order":21,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":25},{"localized_name":"positive","name":"positive","type":"CONDITIONING","link":5},{"localized_name":"negative","name":"negative","type":"CONDITIONING","link":6},{"localized_name":"latent_image","name":"latent_image","type":"LATENT","link":7}],"outputs":[{"localized_name":"LATENT","name":"LATENT","type":"LATENT","links":[11]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"KSampler"},"widgets_values":[3350663858,"randomize",30,3,"euler","normal",1]},{"id":22,"type":"MarkdownNote","pos":[1893.6907958984375,-38.17746353149414],"size":[296.7240295410156,110.05634307861328],"flags":{},"order":8,"mode":0,"inputs":[],"outputs":[],"title":"VAE Decode","properties":{},"widgets_values":["## Takes the latent space image and turns it into normal pixel image"],"color":"#432","bgcolor":"#653"},{"id":27,"type":"MarkdownNote","pos":[2282.317626953125,-26.124797821044922],"size":[296.7240295410156,110.05634307861328],"flags":{},"order":9,"mode":0,"inputs":[],"outputs":[],"title":"Upscale Image","properties":{},"widgets_values":["## Puts the Upscale model into the path for ease of use later"],"color":"#432","bgcolor":"#653"},{"id":12,"type":"ImageScale","pos":[2700.199951171875,130],"size":[315,130],"flags":{},"order":24,"mode":0,"inputs":[{"localized_name":"image","name":"image","type":"IMAGE","link":15}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[16]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"ImageScale"},"widgets_values":["nearest-exact",1792,2304,"disabled"]},{"id":23,"type":"MarkdownNote","pos":[2714.472900390625,-53.508567810058594],"size":[304.4708557128906,138.6599578857422],"flags":{},"order":10,"mode":0,"inputs":[],"outputs":[],"title":"Upscale Image","properties":{},"widgets_values":["## Sets the new image resolution and upscales (adds more detailing) the image. Different methods have their own unique effects, might just be best to stick with nearest-exact"],"color":"#432","bgcolor":"#653"},{"id":24,"type":"MarkdownNote","pos":[3114.924072265625,-33.24766540527344],"size":[296.7240295410156,110.05634307861328],"flags":{},"order":11,"mode":0,"inputs":[],"outputs":[],"title":"VAE Encode","properties":{},"widgets_values":["## Does the reverse of Decode (takes a pixel image and turns it back into latent space)"],"color":"#432","bgcolor":"#653"},{"id":26,"type":"MarkdownNote","pos":[3907.483642578125,-57.084014892578125],"size":[287.785400390625,126.1458740234375],"flags":{},"order":12,"mode":0,"inputs":[],"outputs":[],"title":"VAE Decode","properties":{},"widgets_values":["## Same as the other Decode but more simplified, I've never seen big difference between these two but might be worthwhile to see how things change if you replace them"],"color":"#432","bgcolor":"#653"},{"id":28,"type":"MarkdownNote","pos":[4266.099609375,-9.439356803894043],"size":[280.6344909667969,88],"flags":{},"order":13,"mode":0,"inputs":[],"outputs":[],"title":"Save Image","properties":{},"widgets_values":["## Saves image to output folder in ComfyUI directory on computer"],"color":"#432","bgcolor":"#653"},{"id":25,"type":"MarkdownNote","pos":[3517.163330078125,-92.83856201171875],"size":[331.2867431640625,172.6267547607422],"flags":{},"order":14,"mode":0,"inputs":[],"outputs":[],"title":"KSampler","properties":{},"widgets_values":["## Same as the first. The creator of this workflow probably put this extra one in to add some more unique detailing to the image (through light regenerating based off of the 0.3 denoise). Generally smart and shown to help remove some errors in images and correct hallucinations. Not perfect though."],"color":"#432","bgcolor":"#653"},{"id":21,"type":"MarkdownNote","pos":[1311.921630859375,-541.7207641601562],"size":[554.04833984375,611.7031860351562],"flags":{},"order":15,"mode":0,"inputs":[],"outputs":[],"title":"KSampler","properties":{},"widgets_values":["# This runs through the generation process and creates the image. This is where a lot of number tuning can be done outside of other tools/prompting we might want to use: \n## - seed is a random number that \"seeds\" the latent image, basically giving it random noise so we get unique images. Changing the control after generate to fixed will give us the exact same image assuming we change nothing else in the workflow.\n## - Steps are how many times we generate over an image. Too little spits out latent garbage, too much can lead to a quality image with a lot of hallucinations generally. The less steps the less time it takes to generate an image as well.\n## - CFG is the guidance scale. The higher the number, the more the sampler will do what you tell it. This can be a blessing and a curse, as changing this just slightly higher can have major effects on an image. Could be worthwhile to test how this works against super descriptive prompts vs short\n## - The type of sampler is basically how the sampler calculates the numbers and decides what it should do with a pixel. Changing this can lead to slight changes or a lot depending on the sampler, as well as speed and style. Would recommend dpmpp_2m series\n## - Scheduler is how much noise is removed per step. Basically makes each generation step more aggressive generation/less aggressive. Would recommend karras\n## - Denoise is how much the original reference gets changed. For prompt-to-image this doesn't matter but for image-to-image this is important. 1.00 means create a brand new image, the more you reduce this the more the original idea/structure of the reference image gets preserved"],"color":"#432","bgcolor":"#653"},{"id":10,"type":"UpscaleModelLoader","pos":[100,594],"size":[315,58],"flags":{},"order":16,"mode":0,"inputs":[],"outputs":[{"localized_name":"UPSCALE_MODEL","name":"UPSCALE_MODEL","type":"UPSCALE_MODEL","links":[14]}],"properties":{"cnr_id":"comfy-core","ver":"0.3.27","Node name for S&R":"UpscaleModelLoader"},"widgets_values":["4x_UniversalUpscalerV2-Sharp_101000_G"]},{"id":29,"type":"MarkdownNote","pos":[347.50634765625,-625.24755859375],"size":[712.2214965820312,353.4731750488281],"flags":{},"order":17,"mode":0,"inputs":[],"outputs":[],"title":"GENERAL INFO","properties":{},"widgets_values":["# To download the checkpoints just go to CivitAI and get it from there. The one used in this workflow is linked [here](https://civitai.com/models/277058/epicrealism-xl). You can also find the upscaler that was used by clicking on the Manager button at the top right, then clicking Model Manager and searching for the name of that upscaler (4x_UniversalUpscalerV2) or searching on huggingface/CivitAI for the model. If that proves difficult, filter \"upscale\" through the model manager and you'll find some good ones like RealESRGAN/NMKD/UltraSharp. Stick with the 4x upscalers\n\n# Make sure to put these models in the right folders in the model folder in the ComfyUI directory, will make things a lot easier. Reload the model library (click m) to have access to the newly downloaded files. Sometimes this bugs out and doesn't let you choose the recently downloaded model though. Just reboot ComfyUI should fix it"],"color":"#322","bgcolor":"#533"}],"links":[[1,1,1,2,0,"CLIP"],[2,2,0,3,0,"CLIP"],[3,2,0,4,0,"CLIP"],[5,3,0,5,1,"CONDITIONING"],[6,4,0,5,2,"CONDITIONING"],[7,6,0,5,3,"LATENT"],[8,7,0,8,0,"IMAGE"],[10,1,2,7,1,"VAE"],[11,5,0,9,0,"LATENT"],[12,1,2,9,1,"VAE"],[13,9,0,11,1,"IMAGE"],[14,10,0,11,0,"UPSCALE_MODEL"],[15,11,0,12,0,"IMAGE"],[16,12,0,13,0,"IMAGE"],[17,1,2,13,1,"VAE"],[19,3,0,14,1,"CONDITIONING"],[20,4,0,14,2,"CONDITIONING"],[21,13,0,14,3,"LATENT"],[22,14,0,7,0,"LATENT"],[25,1,0,5,0,"MODEL"],[26,1,0,14,0,"MODEL"]],"groups":[],"config":{},"extra":{"ds":{"scale":0.693433494944183,"offset":[144.29349329875603,781.366213440981]},"ue_links":[]},"version":0.4}